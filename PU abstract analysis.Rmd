---
title: "PU abstract analysis"
author: "JJ"
date: "20/6/2019"
output: html_document
---


```{r}
ipak <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

# usage
packages <- c("ggplot2", "dataMaid","readxl")
ipak(packages)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
test<-read_xlsx(path = "C:/Users/jj.egb/Dropbox (CBS CIU)/Collective Intelligence Unit/7. project 2019 - 2022 (PU)/B_Research/Literature review/Research Areas+concepts/Data scraping/ScopusScrape - Abstract/Data and grpahs.xlsx",sheet = "Data")

##Read files named xyz1111.csv, xyz2222.csv, etc.
filenames <- list.files(path="C:/Users/jj.egb/Dropbox (CBS CIU)/Collective Intelligence Unit/7. project 2019 - 2022 (PU)/B_Research/Literature review/Research Areas+concepts/Data scraping/ScopusScrape - Abstract/Abstract scrapes",pattern="scopus+.*csv")

##Create list of data frame names without the ".csv" part 
names <-substr(filenames,1,10)

###Load all files
for(i in names){
    filepath <- file.path("C:/Users/jj.egb/Dropbox (CBS CIU)/Collective Intelligence Unit/7. project 2019 - 2022 (PU)/B_Research/Literature review/Research Areas+concepts/Data scraping/ScopusScrape - Abstract/Abstract scrapes",paste(i,".csv",sep=""))
    assign(i, read.csv(filepath,sep = ","))
}

#Merge with data
test<-transform(test, Hits = as.numeric(Hits))
test.clean<-test[which(test$Hits>0),]

`scopus(14)`$Keyword<-test$Keyword[which(test$Hits>0)]






```


Clean data
```{r}
DFclean<-test[which(test$Hits>0),]

DFclean[1,"Abstract"]
nchar(DFclean[1,"Abstract"])
```

Topic Modelling - Abstracts
```{r}
#load text mining library
library(tm)
#load files into corpus
#get listing of .txt files in directory
filenames <- paste("C:/Users/jj.egb/Downloads/Textmining/",list.files("C:/Users/jj.egb/Downloads/Textmining",pattern= "*.txt"),sep = "")#tutorial texts

#Create list of abstracts from dataframe
filenames<-(DFclean[,"Abstract"])
filenames <- split(filenames, seq(nrow(filenames)))
filenames <- as.list(as.data.frame(t(filenames)))

#read files into a character vector
files <- lapply(filenames,readLines)

#create corpus from vector
docs <- Corpus(VectorSource(files))
#inspect a particular document in corpus
writeLines(as.character(docs[[30]]))
 

#start preprocessing
#Transform to lower case
docs <-tm_map(docs,content_transformer(tolower))
 

#remove potentially problematic symbols
toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, ” “, x))})
docs <- tm_map(docs, toSpace, “-“)
docs <- tm_map(docs, toSpace, “’”)
docs <- tm_map(docs, toSpace, “‘”)
docs <- tm_map(docs, toSpace, “•”)
docs <- tm_map(docs, toSpace, “””)
docs <- tm_map(docs, toSpace, ““”)


```

