---
title: "PU abstract analysis"
author: "JJ"
date: "20/6/2019"
output: html_document
---


```{r}
ipak <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

# usage
packages <- c("ggplot2", "dataMaid","readxl")
ipak(packages)
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Topic Modelling based on tutorial:
#https://eight2late.wordpress.com/2015/09/29/a-gentle-introduction-to-topic-modeling-using-r/

```{r}

test<-read_xlsx(path = "C:/Users/jj.egb/Dropbox (CBS CIU)/Collective Intelligence Unit/7. project 2019 - 2022 (PU)/B_Research/Literature review/Research Areas+concepts/Data scraping/ScopusScrape - Abstract/Data and grpahs.xlsx",sheet = "Data")#PU Abstracts excel file


test<-read_xlsx(path = "C:/Users/jj.egb/Dropbox (CBS CIU)/Collective Intelligence Unit/7. project 2019 - 2022 (PU)/B_Research/Literature review/Research Areas+concepts/Data scraping/ScopusScrape - Abstract/Data2 - CI Scopus search.xlsx",sheet = "Data")#CI scopus search


#Separate Abstract import 
##Read files named xyz1111.csv, xyz2222.csv, etc.
filenames <- list.files(path="C:/Users/jj.egb/Dropbox (CBS CIU)/Collective Intelligence Unit/7. project 2019 - 2022 (PU)/B_Research/Literature review/Research Areas+concepts/Data scraping/ScopusScrape - Abstract/Abstract scrapes",pattern="scopus+.*csv")
##Create list of data frame names without the ".csv" part 
names <-substr(filenames,1,10)
###Load all files
for(i in names){
    filepath <- file.path("C:/Users/jj.egb/Dropbox (CBS CIU)/Collective Intelligence Unit/7. project 2019 - 2022 (PU)/B_Research/Literature review/Research Areas+concepts/Data scraping/ScopusScrape - Abstract/Abstract scrapes",paste(i,".csv",sep=""))
    assign(i, read.csv(filepath,sep = ","))
}

#Merge with data
test<-transform(test, Hits = as.numeric(Hits))

```


Clean data
```{r}
#Subset DF with only hits
DFclean<-test[which(test$Hits>0),]

#Subset DF with CI abstracts
DFclean<-test[!grepl("No abstract available", test$Abstract),]
#Remove duplicates
DFclean<-DFclean[!duplicated(DFclean$Title),]

#Insight into data
DFclean[2610,"Abstract"]
nchar(DFclean[2610,"Abstract"])


#
anyDuplicated(DFclean$Abstract)
anyNA(test$Title)
```

Topic Modelling - Abstracts
```{r}

#load text mining library
library(SnowballC)
library(tm)

#load files into corpus
#get listing of .txt files in directory
#filenames <- paste("C:/Users/jj.egb/Downloads/Textmining/",list.files("C:/Users/jj.egb/Downloads/Textmining",pattern= "*.txt"),sep = "")#tutorial texts
#read files into a character vector 
#files <- lapply(filenames,readLines) #NB TUTORIAL LOAD

#Custom function
rows.to.list <- function(df ) {
	ll<-apply(df,1,list)
	ll<-lapply(ll,unlist)
}
#Create list of abstract titles from dataframe with custom function
filenames<-rows.to.list(DFclean[,"Title"])
#Create list of abstracts from dataframe with custom function
files<-rows.to.list(DFclean[,"Abstract"])#Abstract Load

#create corpus from vector
docs <- VCorpus(VectorSource(files))
#inspect a particular document in corpus
writeLines(as.character(docs[[2]]))


#start preprocessing
#Transform to lower case
docs <-tm_map(docs,content_transformer(tolower))
 

#remove potentially problematic symbols
toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))})
docs <- tm_map(docs, toSpace, "-")
docs <- tm_map(docs, toSpace, "’")
docs <- tm_map(docs, toSpace, "‘")
docs <- tm_map(docs, toSpace, "•")
docs <- tm_map(docs, toSpace, '"')
docs <- tm_map(docs, toSpace, '@')
docs <- tm_map(docs, toSpace, '©')
docs <- tm_map(docs, toSpace, '“')
docs <- tm_map(docs, toSpace, '”')
docs <- tm_map(docs, toSpace, '–')







#remove punctuation
docs <- tm_map(docs, removePunctuation)
#Strip digits
docs <- tm_map(docs, removeNumbers)
#remove stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
#remove whitespace
docs <- tm_map(docs, stripWhitespace)
#Good practice to check every now and then
writeLines(as.character(docs[[2]]))
#Stem document
docs <- tm_map(docs,stemDocument)
 

#fix up 1) differences between us and aussie english 2) general errors
docs <- tm_map(docs, content_transformer(gsub),
pattern = "organiz", replacement = "organ")
docs <- tm_map(docs, content_transformer(gsub),
pattern = "organis", replacement = "organ")
docs <- tm_map(docs, content_transformer(gsub),
pattern = "andgovern", replacement = "govern")
docs <- tm_map(docs, content_transformer(gsub),
pattern = "inenterpris", replacement = "enterpris")
docs <- tm_map(docs, content_transformer(gsub),
pattern = "team-", replacement = "team")
docs <- tm_map(docs, content_transformer(gsub),
pattern = "participatori", replacement = "particip")
#define and eliminate all custom stopwords
myStopwords <- c("can", "say","one","way","use",
"also","howev","tell","will",
"much","need","take","tend","even",
"like","particular","rather","said",
"get","well","make","ask","come","end",
"first","two","help","often","may",
"might","see","someth","thing","point",
"post","look","right","now","think","‘ve ",
"‘re ","anoth","put","set","new","good",
"want","sure","kind","larg","yes,","day","etc",
"quit","sinc","attempt","lack","seen","awar",
"littl","ever","moreov","though","found","abl",
"enough","far","earli","away","achiev","draw",
"last","never","brief","bit","entir","brief",
"great","lot","collect","intellig","contrib","paper")
docs <- tm_map(docs, removeWords, myStopwords)
#inspect a document as a check
writeLines(as.character(docs[[2]]))
 
#Create document-term matrix
dtm <- DocumentTermMatrix(docs)

#convert rownames to filenames
rownames(dtm) <- filenames

#Remove Empty entries
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm   <- dtm[rowTotals> 0, ]           #remove all docs without words

#collapse matrix by summing over columns
freq <- colSums(as.matrix(dtm))
#length should be total number of terms
length(freq)
#create sort order (descending)
ord <- order(freq,decreasing=TRUE)
#List all terms in decreasing order of freq and write to disk
head(freq[ord])
write.csv(freq[ord],"word_freq.csv")

```

LDA Topic Modelling
```{r}
#load topic models library
library(topicmodels)
 
#Set parameters for Gibbs sampling
burnin <- 4000
iter <- 2000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE

#Number of topics
k <- 2
#That done, we can now do the actual work – run the topic modelling algorithm on our corpus. Here is the code:

#Run LDA using Gibbs sampling - with timers
start_time <- Sys.time()
ldaOut <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))
end_time <- Sys.time()
end_time - start_time

#write out results
#docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
write.csv(ldaOut.topics,file=paste("LDAGibbs",k,"DocsToTopics.csv"))
 

#top 6 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,6))
write.csv(ldaOut.terms,file=paste("LDAGibbs",k,"TopicsToTerms.csv"))
ldaOut.terms 

#probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file=paste("LDAGibbs",k,"TopicProbabilities.csv"))
 

#Find relative importance of top 2 topics
topic1ToTopic2 <- lapply(1:nrow(dtm),function(x)
sort(topicProbabilities[x,])[k]/sort(topicProbabilities[x,])[k-1])
 

#Find relative importance of second and third most important topics
topic2ToTopic3 <- lapply(1:nrow(dtm),function(x)
sort(topicProbabilities[x,])[k-1]/sort(topicProbabilities[x,])[k-2])
 

#write to file
write.csv(topic1ToTopic2,file=paste("LDAGibbs",k,"Topic1ToTopic2.csv"))
write.csv(topic2ToTopic3,file=paste("LDAGibbs",k,"Topic2ToTopic3.csv"))
```

